---
title: 'Practical Machine Learning Week 4 Quiz'
author: "Claus Bo Hansen"
date: "December 25, 2018"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(caret)
library(dplyr)
#library(Hmisc)
#library(ggplot2)
#library(rpart)
#library(rpart.plot)
#library(rattle)
#library(pgmm)

```

## Question 1

If you aren't using these versions of the packages, your answers may not exactly match the right answer, but hopefully should be close.

Load the vowel.train and vowel.test data sets:

```{r, echo = TRUE}



library(ElemStatLearn)

data(vowel.train)

data(vowel.test)


# Set the variable y to be a factor variable in both the training and test set.
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)

# Then set the seed to 33833.
set.seed(33833)

# Fit (1) a random forest predictor relating the factor variable y to the remaining variables and (2) a boosted predictor using the "gbm" method.
# Fit these both with the train() command in the caret package. 
model_rf <- train(y ~ ., data = vowel.train, method = "rf")
model_gbm <- train(y ~ ., data = vowel.train, method = "gbm")

prediction_rf <- predict(model_rf, vowel.test)
prediction_gbm <- predict(model_gbm, vowel.test)

confusionMatrix(prediction_rf, vowel.test$y)
confusionMatrix(prediction_gbm, vowel.test$y)
confusionMatrix(prediction_rf, prediction_gbm)

```

RF: Accuracy : 0.5996
GBM: Accuracy : 0.5173
Agreement: 0.6797

Solution:
RF Accuracy = 0.6082

GBM Accuracy = 0.5152

Agreement Accuracy = 0.6361

## Question 2

Load the Alzheimer's data using the following commands

```{r, echo = TRUE}

library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

# Set the seed to 62433
set.seed(62433)

# and predict diagnosis with all the other variables using a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis ("lda") model.
model_rf <- train(diagnosis ~., data = training, model = "rf")
model_gbm <- train(diagnosis ~., data = training, model = "gbm")
model_lda <- train(diagnosis ~., data = training, model = "lda")

# Individual predictions on test data
prediction_rf <- predict(model_rf, testing)
prediction_gbm <- predict(model_gbm, testing)
prediction_lda <- predict(model_lda, testing)

# Individual predictions on train data
train_prediction_rf <- predict(model_rf, training)
train_prediction_gbm <- predict(model_gbm, training)
train_prediction_lda <- predict(model_lda, training)


# Stack the predictions together using random forests ("rf").
stack_train <- as.data.frame(cbind(training$diagnosis, train_prediction_rf, train_prediction_gbm, train_prediction_lda)) %>%
  dplyr::rename(diagnosis = V1, rf = train_prediction_rf, gbm = train_prediction_gbm, lda = train_prediction_lda)
stack_train$diagnosis <- as.factor(stack_train$diagnosis)

model_stacked <- train(diagnosis ~ ., data = stack_train, model = "rf")

stack_test <- as.data.frame(cbind(testing$diagnosis, prediction_rf, prediction_gbm, prediction_lda)) %>%
  dplyr::rename(diagnosis = V1, rf = prediction_rf, gbm = prediction_gbm, lda = prediction_lda)
stack_test$diagnosis <- as.factor(stack_test$diagnosis)

prediction_stacked <- predict(model_stacked, stack_test)
levels(prediction_stacked) <- c("Impaired", "Control")

# What is the resulting accuracy on the test set?
confusionMatrix(prediction_stacked, testing$diagnosis)
confusionMatrix(prediction_rf, testing$diagnosis)
confusionMatrix(prediction_gbm, testing$diagnosis)
confusionMatrix(prediction_lda, testing$diagnosis)

# Is it better or worse than each of the individual predictions? 


```


prediction_stacked: Accuracy : 0.7927
prediction_rf: Accuracy : 0.7927
prediction_gbm: Accuracy : 0.7927
prediction_lda: Accuracy : 0.8049

No matching answer, assuming

Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.


## Question 3

Load the concrete data with the commands:

```{r, echo = TRUE}

library(AppliedPredictiveModeling)
library(elasticnet)

set.seed(3523)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]

# Set the seed to 233
set.seed(233)

# and fit a lasso model to predict Compressive Strength.
# Which variable is the last coefficient to be set to zero as the penalty increases? (Hint: it may be useful to look up ?plot.enet). 

model_lasso <- train(CompressiveStrength ~ ., data = training, method = "lasso")

plot.enet(model_lasso$finalModel, xvar = "penalty", use.color = TRUE)

```

Answer: Cement


## Question 4

Load the data on the number of visitors to the instructors blog from here:

https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv

Using the commands:


```{r, echo = TRUE}

library(forecast)

library(lubridate) # For year() function below
dat = read.csv("/home/sbo/Data/Nextcloud/Documents/Data Science/Coursera-MachineLearning/Practical Machine Learning/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)

# Fit model
model_fitted <- bats(tstrain)

forecast <- forecast(model_fitted, nrow(testing), level = c(95))
plot(forecast)
points(dat$visitsTumblr)

sum(forecast$lower <= testing$visitsTumblr & forecast$upper >= testing$visitsTumblr) / dim(testing)[1]


```

Points between 95% lower and upper prediction interval: 0.9617021

Answer: 96%

## Question 5

Load the concrete data with the commands:


```{r, echo = TRUE}

library(e1071)

set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]

# Set the seed to 325
set.seed(325)

# and fit a support vector machine using the e1071 package to predict Compressive Strength using the default settings.
model_fitted <- svm(CompressiveStrength ~ ., data = training)

# Predict on the testing set.
prediction <- predict(model_fitted, testing)

# What is the RMSE?
errors <- prediction - testing$CompressiveStrength

sqrt(mean(errors^2))

```

RMSE = 6.715009






